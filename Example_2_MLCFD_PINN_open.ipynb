{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41monster/ML_Spring_2023/blob/main/Example_2_MLCFD_PINN_open.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bl0k1tqDZOU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Input, Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQtnkRPKDZOV"
      },
      "outputs": [],
      "source": [
        "# preprocessing of training dataset\n",
        "# CFD data(excel) load\n",
        "filename = '/content/drive/My Drive/Colab Notebooks/Dataset_example_1.00_1.02sec_3input.xlsx'\n",
        "pre_dataset = pd.read_excel(filename)\n",
        "\n",
        "# train_stats for normalization\n",
        "train_stats = pre_dataset.describe()\n",
        "train_stats.pop(\"p\")\n",
        "train_stats.pop(\"ux\")\n",
        "train_stats.pop(\"uy\")\n",
        "train_stats = train_stats.transpose()\n",
        "\n",
        "# train label\n",
        "dataset = pre_dataset\n",
        "train_labels_p = dataset.pop(\"p\")\n",
        "train_labels_u = dataset.pop(\"ux\")\n",
        "train_labels_v = dataset.pop(\"uy\")\n",
        "\n",
        "# concat train labels (for subclass model)\n",
        "train_labels_p_re = tf.reshape(train_labels_p, shape=[-1,1])\n",
        "train_labels_u_re = tf.reshape(train_labels_u, shape=[-1,1])\n",
        "train_labels_v_re = tf.reshape(train_labels_v, shape=[-1,1])\n",
        "train_labels = np.concatenate((train_labels_p_re, train_labels_u_re,train_labels_v_re),axis=1)\n",
        "\n",
        "# normalization\n",
        "def norm(x):\n",
        "    return (x-train_stats['mean'])/train_stats['std']\n",
        "normed_train_data = norm(dataset)\n",
        "normed_train_data_re= tf.reshape(normed_train_data, shape=[-1,15])\n",
        "dataset_re = tf.reshape(dataset, shape=[-1,15])\n",
        "\n",
        "# input data (normed, raw)\n",
        "#x = tf.concat([normed_train_data_re, dataset_re[:,0:1], dataset_re[:,5:6], dataset_re[:,10:11]],axis=1)\n",
        "x_sum = np.concatenate((normed_train_data_re, dataset_re[:,0:1], dataset_re[:,5:6], dataset_re[:,10:11]),axis=1)\n",
        "x_sum_train = x_sum[0:18000,:] # sometimes dataset size can be bigger than 18000\n",
        "# output data\n",
        "y_sum = train_labels\n",
        "y_sum_train = train_labels[0:18000,:]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x, x_val, y, y_val = train_test_split(x_sum_train,y_sum_train,test_size=0.2, random_state = 1004)\n",
        "x_pinn = x_sum[0:18000,:]\n",
        "y_pinn = y_sum[0:18000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjbKxgSeDZOW"
      },
      "outputs": [],
      "source": [
        "# build 6 individual network models\n",
        "# network architecture was determined in previous optimization study\n",
        "net_p = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(64, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)   \n",
        "    ],\n",
        "    name=\"net_p\",\n",
        ")\n",
        "\n",
        "net_u = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(64, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)   \n",
        "    ],\n",
        "    name=\"net_u\",\n",
        ")\n",
        "\n",
        "net_v = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(64, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)   \n",
        "    ],\n",
        "    name=\"net_v\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEOeNxpIDZOX"
      },
      "outputs": [],
      "source": [
        "# loss_tracker for monotoring during training\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "loss_tracker_2 = keras.metrics.Mean(name=\"loss\")\n",
        "loss_tracker_3 = keras.metrics.Mean(name=\"loss\")\n",
        "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
        "\n",
        "# PINN algorithm\n",
        "class FVMN_PINNs(keras.Model):\n",
        "    def __init__(self, net_p, net_u, net_v, Rs, x_val=x_val, y_val=y_val, x_pinn=x_pinn, y_pinn=y_pinn):\n",
        "        super(FVMN_PINNs, self).__init__()          \n",
        "        self.net_p = net_p\n",
        "        self.net_u = net_u\n",
        "        self.net_v = net_v\n",
        "        self.Rs = Rs\n",
        "        self.x_val = x_val\n",
        "        self.y_val = y_val\n",
        "        self.x_pinn = x_pinn\n",
        "        self.y_pinn = y_pinn\n",
        "    \n",
        "    def call(self,x):\n",
        "        x = self.net_p(x)\n",
        "        x = self.net_u(x)\n",
        "        x = self.net_v(x)\n",
        " \n",
        "    def train_step(self, data):\n",
        "        x_pre, y = data\n",
        "        # the order of the variables depends on the dataset.\n",
        "        x = x_pre[:,0:15]\n",
        "        x_ex = x_pre[:,15:18]\n",
        "        x_ex = tf.cast(x_ex, dtype='float32')\n",
        "        \n",
        "        y_p = y[:,0:1]\n",
        "        y_u = y[:,1:2] \n",
        "        y_v = y[:,2:3]\n",
        "         \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            #multi grads calculation: persistent=True\n",
        "            y_pred_p = self.net_p(x)\n",
        "            y_pred_u = self.net_u(x)  # Forward pass\n",
        "            y_pred_v = self.net_v(x)\n",
        "            \n",
        "            # Calculate loss in batches.\n",
        "            loss_p_mse = tf.reduce_mean(tf.square(y_p-y_pred_p), axis = 0)\n",
        "            loss_u_mse = tf.reduce_mean(tf.square(y_u-y_pred_u), axis = 0)\n",
        "            loss_v_mse = tf.reduce_mean(tf.square(y_v-y_pred_v), axis = 0)\n",
        "                       \n",
        "            # loss for training\n",
        "            loss_p = loss_p_mse\n",
        "            loss_u = loss_u_mse\n",
        "            loss_v = loss_v_mse\n",
        "            \n",
        "            loss_tot = loss_p + loss_u + loss_v\n",
        "        \n",
        "            # conservation loss \n",
        "            x_pinn_pre = self.x_pinn\n",
        "            y_pinn = self.y_pinn           \n",
        "            x_pinn = x_pinn_pre[:,0:15]\n",
        "            x_pinn_ex = x_pinn_pre[:,15:18]\n",
        "        \n",
        "            #multi grads calculation: persistent=True\n",
        "            y_pred_pinn_p = self.net_p(x_pinn)\n",
        "            y_pred_pinn_u = self.net_u(x_pinn)\n",
        "            y_pred_pinn_v = self.net_v(x_pinn)\n",
        "            \n",
        "            # Residual error calculation\n",
        "            y_pred_pinn_p_ex = y_pred_pinn_p + x_pinn_ex[:,0:1]\n",
        "            y_pred_pinn_u_ex = y_pred_pinn_u + x_pinn_ex[:,1:2]\n",
        "            y_pred_pinn_v_ex = y_pred_pinn_v + x_pinn_ex[:,2:3]\n",
        "        \n",
        "            y_pred_pinn_p_tf = tf.reshape(y_pred_pinn_p_ex, [180,100]) #180\n",
        "            y_pred_pinn_u_tf = tf.reshape(y_pred_pinn_u_ex, [180,100])\n",
        "            y_pred_pinn_v_tf = tf.reshape(y_pred_pinn_v_ex, [180,100])\n",
        "            \n",
        "            # continuity residual error (incompressible flow)\n",
        "            y_pred_pinn_u_diff = y_pred_pinn_u_tf[2:180,1:-1] - y_pred_pinn_u_tf[0:178,1:-1] \n",
        "            y_pred_pinn_v_diff = y_pred_pinn_v_tf[1:179,2:] - y_pred_pinn_v_tf[1:179,:-2]\n",
        "            \n",
        "            # momentum balance residual error (incompressible flow)\n",
        "            y_mom_1_tf = tf.reshape(y_pred_pinn_u, [180,100])\n",
        "            y_mom_2_tf = tf.reshape(y_pred_pinn_v, [180,100])\n",
        "        \n",
        "            y_mom_1 = y_mom_1_tf[1:179,1:-1]\n",
        "            y_mom_2 = y_mom_2_tf[1:179,1:-1]\n",
        "            y_mom_3_1 = y_pred_pinn_u_tf[1:179,1:-1]\n",
        "            y_mom_3_2 = y_pred_pinn_u_tf[2:180,1:-1] - y_pred_pinn_u_tf[0:178,1:-1] + y_pred_pinn_v_tf[2:180,1:-1] - y_pred_pinn_v_tf[0:178,1:-1]\n",
        "            y_mom_3 = y_mom_3_1*y_mom_3_2\n",
        "            y_mom_4_1 = y_pred_pinn_v_tf[1:179,1:-1]\n",
        "            y_mom_4_2 = y_pred_pinn_v_tf[1:179,2:] - y_pred_pinn_v_tf[1:179,:-2] + y_pred_pinn_u_tf[1:179,2:] - y_pred_pinn_u_tf[1:179,:-2]\n",
        "            y_mom_4 = y_mom_4_1*y_mom_4_2\n",
        "            y_mom_5_1 = y_pred_pinn_u_tf[2:180,1:-1]-2*y_pred_pinn_u_tf[1:179,1:-1]+y_pred_pinn_u_tf[0:178,1:-1]+y_pred_pinn_v_tf[2:180,1:-1]-2*y_pred_pinn_v_tf[1:179,1:-1]+y_pred_pinn_v_tf[0:178,1:-1]\n",
        "            y_mom_5_2 = y_pred_pinn_u_tf[1:179,2:]-2*y_pred_pinn_u_tf[1:179,1:-1]+y_pred_pinn_u_tf[1:179,:-2]+ y_pred_pinn_v_tf[1:179,2:]-2*y_pred_pinn_v_tf[1:179,1:-1]+y_pred_pinn_v_tf[1:179,:-2]\n",
        "            y_mom_5 = 0.01*(y_mom_5_1+y_mom_5_2)\n",
        "            y_mom_6 = y_pred_pinn_p_tf[2:180,1:-1] - y_pred_pinn_p_tf[0:178,1:-1] + y_pred_pinn_p_tf[1:179,2:] - y_pred_pinn_p_tf[1:179,:-2]\n",
        "            \n",
        "            y_mom_tot_1 = y_mom_1 + y_mom_2 + y_mom_3/2 + y_mom_4/2 - y_mom_5*100 + y_mom_6/2\n",
        "        \n",
        "            # due to uniform mesh\n",
        "            Rs_c_1 = y_pred_pinn_u_diff + y_pred_pinn_v_diff\n",
        "            Rs_c = Rs_c_1\n",
        "            Rs_m = y_mom_tot_1\n",
        "    \n",
        "            Rs_tot_c = tf.reduce_mean(tf.square(Rs_c), axis = 0)\n",
        "            Rs_tot_m = tf.reduce_mean(tf.square(Rs_m), axis = 0)\n",
        "            Rs_tot = Rs_tot_c*0.1 + Rs_tot_m #0.1은 Rs_c와의 scale 보정\n",
        "            \n",
        "            loss_p_pinn = loss_p+ Rs_tot*0.1\n",
        "            loss_u_pinn = loss_u+ Rs_tot*0.1\n",
        "            loss_v_pinn = loss_v+ Rs_tot*0.1\n",
        "            \n",
        "            loss_tot_pinn = loss_p_pinn + loss_u_pinn + loss_v_pinn\n",
        "        \n",
        "        # Compute gradients\n",
        "        trainable_vars_p = self.net_p.trainable_variables\n",
        "        trainable_vars_u = self.net_u.trainable_variables\n",
        "        trainable_vars_v = self.net_v.trainable_variables \n",
        "        \n",
        "        gradients_p = tape.gradient(loss_p_pinn, trainable_vars_p)\n",
        "        gradients_u = tape.gradient(loss_u_pinn, trainable_vars_u)\n",
        "        gradients_v = tape.gradient(loss_v_pinn, trainable_vars_v)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients_p, trainable_vars_p))\n",
        "        self.optimizer.apply_gradients(zip(gradients_u, trainable_vars_u))\n",
        "        self.optimizer.apply_gradients(zip(gradients_v, trainable_vars_v))\n",
        "        \n",
        "        #validation loss\n",
        "        x_val = self.x_val\n",
        "        y_val = self.y_val \n",
        "\n",
        "        x_val_2 = x_val[:,0:15]      \n",
        "        y_val_p = y_val[:,0:1]\n",
        "        y_val_u = y_val[:,1:2] \n",
        "        y_val_v = y_val[:,2:3]\n",
        "        \n",
        "        y_val_pred_p = self.net_p(x_val_2)\n",
        "        y_val_pred_u = self.net_u(x_val_2)  # Forward pass\n",
        "        y_val_pred_v = self.net_v(x_val_2)\n",
        "    \n",
        "        loss_val_p_mse = tf.reduce_mean(tf.square(y_val_p-y_val_pred_p), axis = 0)\n",
        "        loss_val_u_mse = tf.reduce_mean(tf.square(y_val_u-y_val_pred_u), axis = 0)\n",
        "        loss_val_v_mse = tf.reduce_mean(tf.square(y_val_v-y_val_pred_v), axis = 0)\n",
        "        \n",
        "        val_loss = loss_val_p_mse + loss_val_u_mse + loss_val_v_mse\n",
        "    \n",
        "        #validation loss = validation loss + conservation error\n",
        "        #Rs = self.Rs\n",
        "        #val_loss = val_loss_pre + Rs_tot*Rs\n",
        "    \n",
        "        # loss, mse for epochs monitoring\n",
        "        #loss_tracker.update_state(loss_tot)\n",
        "        loss_tracker.update_state(val_loss) #Rs_tot\n",
        "        loss_tracker_2.update_state(Rs_tot)\n",
        "        loss_tracker_3.update_state(loss_tot)\n",
        "\n",
        "        #mae_metric.update_state(y_u, y_pred_u)            \n",
        "        #return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}          \n",
        "        return {\"val_loss\": loss_tracker.result(), \"Rs_tot\": loss_tracker_2.result(), \"loss_tot\": loss_tracker_3.result()}\n",
        "\n",
        "    @property\n",
        "    #1epoch 1 initialization: https://stackoverflow.com/questions/57248723/why-there-is-sudden-drop-in-loss-after-every-epoch\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, loss_tracker_2, loss_tracker_3] # `reset_states()` can be called automatically at the start of each epoch\n",
        "\n",
        "import os\n",
        "checkpoint_path =  './MLCFD-PINN_test'\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                   save_weights_only=True, save_best_only=True, monitor = 'val_loss',\n",
        "                                   verbose=1)\n",
        "    \n",
        "# model fit\n",
        "model = FVMN_PINNs(net_p=net_p, net_u=net_u,net_v=net_v, Rs =0.001, x_val=x_val, y_val=y_val, x_pinn = x_pinn,y_pinn = y_pinn )\n",
        "\n",
        "model.compile(optimizer=\"adam\")\n",
        "history = model.fit(x, y,batch_size=100, epochs=100, callbacks=[cp_callback]) # no validation data arguments"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}